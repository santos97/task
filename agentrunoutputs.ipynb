{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14156682,"sourceType":"datasetVersion","datasetId":9017998}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"--- Installing project dependencies ---\")\n!pip install -U langchain langchain-community sentence-transformers faiss-cpu pydantic pandas langchain-huggingface\n\nprint(\"--- Verification ---\")\n!pip list | grep -E \"numpy|scipy|scikit-learn|faiss|langchain\"\n!pip install -U langchain-openai\nprint(\"Installation complete. The kernel MUST be restarted now.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-15T11:11:32.191397Z","iopub.execute_input":"2025-12-15T11:11:32.191594Z","iopub.status.idle":"2025-12-15T11:13:31.884257Z","shell.execute_reply.started":"2025-12-15T11:11:32.191574Z","shell.execute_reply":"2025-12-15T11:13:31.882015Z"}},"outputs":[{"name":"stdout","text":"--- Installing project dependencies ---\nRequirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.27)\nCollecting langchain\n  Downloading langchain-1.1.3-py3-none-any.whl.metadata (4.9 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting sentence-transformers\n  Downloading sentence_transformers-5.2.0-py3-none-any.whl.metadata (16 kB)\nCollecting faiss-cpu\n  Downloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.12.4)\nCollecting pydantic\n  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nCollecting pandas\n  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting langchain-huggingface\n  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting langchain-core<2.0.0,>=1.1.2 (from langchain)\n  Downloading langchain_core-1.2.0-py3-none-any.whl.metadata (3.7 kB)\nCollecting langgraph<1.1.0,>=1.0.2 (from langchain)\n  Downloading langgraph-1.0.5-py3-none-any.whl.metadata (7.4 kB)\nCollecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.13.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.11.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.3)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nRequirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\nCollecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.1.2->langchain) (1.33)\nCollecting uuid-utils<1.0,>=0.12.0 (from langchain-core<2.0.0,>=1.1.2->langchain)\n  Downloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\nCollecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\nCollecting langgraph-sdk<0.4.0,>=0.3.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n  Downloading langgraph_sdk-0.3.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.10.5)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain) (3.0.0)\nCollecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n  Downloading ormsgpack-1.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\nDownloading langchain-1.1.3-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading sentence_transformers-5.2.0-py3-none-any.whl (493 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading faiss_cpu-1.13.1-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\nDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-1.2.0-py3-none-any.whl (475 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.9/475.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph-1.0.5-py3-none-any.whl (157 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.1/157.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\nDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\nDownloading langgraph_sdk-0.3.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uuid_utils-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.7/343.7 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ormsgpack-1.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.3/211.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: uuid-utils, ormsgpack, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-huggingface, langgraph-prebuilt, langchain-classic, langgraph, langchain, sentence-transformers, pandas, langchain-community, faiss-cpu\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.12.4\n    Uninstalling pydantic-2.12.4:\n      Successfully uninstalled pydantic-2.12.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.72\n    Uninstalling langchain-core-0.3.72:\n      Successfully uninstalled langchain-core-0.3.72\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.9\n    Uninstalling langchain-text-splitters-0.3.9:\n      Successfully uninstalled langchain-text-splitters-0.3.9\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.27\n    Uninstalling langchain-0.3.27:\n      Successfully uninstalled langchain-0.3.27\n  Attempting uninstall: sentence-transformers\n    Found existing installation: sentence-transformers 4.1.0\n    Uninstalling sentence-transformers-4.1.0:\n      Successfully uninstalled sentence-transformers-4.1.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.3\n    Uninstalling pandas-2.2.3:\n      Successfully uninstalled pandas-2.2.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed faiss-cpu-1.13.1 langchain-1.1.3 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.2.0 langchain-huggingface-1.2.0 langchain-text-splitters-1.1.0 langgraph-1.0.5 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ormsgpack-1.12.1 pandas-2.3.3 pydantic-2.12.5 sentence-transformers-5.2.0 uuid-utils-0.12.0\n--- Verification ---\nfaiss-cpu                                1.13.1\nlangchain                                1.1.3\nlangchain-classic                        1.0.0\nlangchain-community                      0.4.1\nlangchain-core                           1.2.0\nlangchain-huggingface                    1.2.0\nlangchain-text-splitters                 1.1.0\nnumpy                                    1.26.4\nscikit-learn                             1.2.2\nscikit-learn-intelex                     2025.9.0\nscipy                                    1.15.3\nCollecting langchain-openai\n  Downloading langchain_openai-1.1.3-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: langchain-core<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.2.0)\nRequirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (2.7.1)\nRequirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-openai) (1.33)\nRequirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-openai) (0.4.8)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-openai) (25.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-openai) (2.12.5)\nRequirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-openai) (6.0.3)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-openai) (9.1.2)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-openai) (4.15.0)\nRequirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.1.3->langchain-openai) (0.12.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.11.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.10.0)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.32.5)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.3->langchain-openai) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-openai) (3.11.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-openai) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.3->langchain-openai) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.3->langchain-openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.3->langchain-openai) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.3->langchain-openai) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain-openai) (2.5.0)\nDownloading langchain_openai-1.1.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: langchain-openai\nSuccessfully installed langchain-openai-1.1.3\nInstallation complete. The kernel MUST be restarted now.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Add LLM Key to notebook","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"groqkey\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T11:13:36.761806Z","iopub.execute_input":"2025-12-15T11:13:36.762151Z","iopub.status.idle":"2025-12-15T11:13:36.916957Z","shell.execute_reply.started":"2025-12-15T11:13:36.762115Z","shell.execute_reply":"2025-12-15T11:13:36.915854Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Add embedding model from hugging face","metadata":{}},{"cell_type":"code","source":"from langchain_community.embeddings import HuggingFaceEmbeddings\n\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nembeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T11:13:39.731210Z","iopub.execute_input":"2025-12-15T11:13:39.731540Z","iopub.status.idle":"2025-12-15T11:14:51.724178Z","shell.execute_reply.started":"2025-12-15T11:13:39.731516Z","shell.execute_reply":"2025-12-15T11:14:51.723035Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/2354019148.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n2025-12-15 11:14:08.457153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765797248.714559      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765797248.790197      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb8553127f64bf9b0dd882ebb4567b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6393d76f4694d04b3a56599b026dcc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69e1c0552c5c4e138f43b495188ee6da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2c527cdb2fd4493ba580d84ff12767c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1402deacb2ec4f188806f7927b94039d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d465fa564974ac5a56fb12b8dc3859d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0bacfc554794e2aa4a267d66d965ec6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65fc6ace39a647c1971b5e0f1eadf34c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2338da6a9f7649a39c0136b3fd431017"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a940f07aae764f03a234660248ffa5bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00a789309551412dae7a6d6635ecff50"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport pandas as pd\n\nfrom langchain_core.documents import Document\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_openai import ChatOpenAI\n\n\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n\n\nFAISS_TEXT_DIR = Path(\"/kaggle/input/consigli/data/processed/index/faiss_text\")\nFAISS_TABLES_DIR = Path(\"/kaggle/input/consigli/data/processed/index/faiss_tables\")\n\nGROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\nGROQ_MODEL_NAME = \"openai/gpt-oss-120b\"\n\nMAX_CHAT_HISTORY_TURNS = 8  \n\nNUMERIC_KEYWORDS = {\n    \"revenue\",\n    \"profit\",\n    \"profits\",\n    \"net income\",\n    \"income\",\n    \"ebit\",\n    \"ebitda\",\n    \"margin\",\n    \"sales\",\n    \"growth\",\n}\n\ndef normalize_text(text: str) -> str:\n    return re.sub(r\"\\s+\", \" \", text).strip()\n\n\ndef is_numeric_query(query: str) -> bool:\n    q = query.lower()\n    return any(k in q for k in NUMERIC_KEYWORDS)\n\n\ndef extract_target_years(query: str) -> List[int]:\n    \"\"\"Years mentioned in the question (may be older than available report PDFs).\"\"\"\n    return [int(y) for y in re.findall(r\"\\b(20\\d{2})\\b\", query)]\n\n\ndef extract_companies(query: str) -> List[str]:\n    \"\"\"Extract company mentions from a query.\"\"\"\n    q = query.lower()\n    companies = []\n    if \"bmw\" in q:\n        companies.append(\"BMW\")\n    if \"ford\" in q:\n        companies.append(\"Ford\")\n    if \"tesla\" in q:\n        companies.append(\"Tesla\")\n    return companies\n\n\ndef parse_context_only_company(user_message: str) -> Optional[str]:\n    \"\"\"\n    Detect messages that are essentially just setting context\n    Returns the company if detected, else None.\n    \"\"\"\n    msg = normalize_text(user_message).lower()\n\n    patterns = [\n        r\"^(it'?s\\s+for\\s+)(tesla|bmw|ford)$\",\n        r\"^(for\\s+)(tesla|bmw|ford)$\",\n        r\"^(tesla|bmw|ford)$\",\n        r\"^(tesla|bmw|ford)\\s+(please|pls)$\",\n    ]\n\n    for pat in patterns:\n        m = re.match(pat, msg)\n        if m:\n            company_token = m.group(len(m.groups()))  # last group is company\n            return company_token.capitalize() if company_token != \"bmw\" else \"BMW\"\n\n    if \"tesla\" in msg and len(msg.split()) <= 6 and (\"for\" in msg or \"its\" in msg or \"it's\" in msg):\n        return \"Tesla\"\n    if \"bmw\" in msg and len(msg.split()) <= 6 and (\"for\" in msg or \"its\" in msg or \"it's\" in msg):\n        return \"BMW\"\n    if \"ford\" in msg and len(msg.split()) <= 6 and (\"for\" in msg or \"its\" in msg or \"it's\" in msg):\n        return \"Ford\"\n\n    return None\n\n\ndef rewrite_query_with_active_context(\n    user_query: str,\n    active_company: Optional[str],\n    active_target_years: List[int],\n) -> str:\n    \"\"\"\n    If the user query doesn't name a company, inject the active company.\n    If the user query doesn't include any years but we have active_target_years, optionally inject them.\n    This improves retrieval and helps the LLM behave like it \"remembers\" the context.\n    \"\"\"\n    rewritten = user_query.strip()\n\n    if active_company and not extract_companies(rewritten):\n        rewritten = f\"{active_company}: {rewritten}\"\n\n    if active_target_years and not extract_target_years(rewritten):\n        if is_numeric_query(rewritten) or \"compare\" in rewritten.lower() or \"trend\" in rewritten.lower():\n            years_str = \", \".join(str(y) for y in active_target_years)\n            rewritten = f\"{rewritten} (years: {years_str})\"\n\n    return rewritten\n\n\ndef build_company_report_years_map(text_store: FAISS) -> Dict[str, List[int]]:\n    years_map: Dict[str, set] = {}\n\n    doc_dict = getattr(text_store.docstore, \"_dict\", {})\n    for doc in doc_dict.values():\n        meta = doc.metadata or {}\n        company = meta.get(\"company\")\n        report_year = meta.get(\"report_year\")\n        if company and isinstance(report_year, int):\n            years_map.setdefault(company, set()).add(report_year)\n\n    return {c: sorted(list(yrs)) for c, yrs in years_map.items()}\n\n\ndef build_retrieval_plan(\n    user_query: str,\n    convo_state: Dict[str, Any],\n    company_report_years: Dict[str, List[int]],\n) -> Dict[str, Any]:\n    \"\"\"\n    Distinguish:\n    - target_years: years user asked about (can be 2017)\n    - report_years_to_search: actual PDF report years we have (2021–2023)\n    \"\"\"\n    companies = extract_companies(user_query) or ([convo_state[\"active_company\"]] if convo_state.get(\"active_company\") else [])\n    target_years = extract_target_years(user_query) or convo_state.get(\"active_target_years\", [])\n\n    is_numeric = is_numeric_query(user_query)\n    needs_comparison = len(companies) > 1 or len(target_years) > 1\n\n    company_to_report_years: Dict[str, List[int]] = {}\n    for company in companies:\n        available_years = company_report_years.get(company, [])\n\n        if not available_years:\n            company_to_report_years[company] = [None]\n            continue\n\n        requested_report_years = [y for y in target_years if y in available_years]\n        if requested_report_years:\n            company_to_report_years[company] = sorted(requested_report_years)\n        else:\n            company_to_report_years[company] = available_years\n\n    return {\n        \"is_numeric\": is_numeric,\n        \"companies\": companies,\n        \"target_years\": target_years,\n        \"company_to_report_years\": company_to_report_years,\n        \"needs_comparison\": needs_comparison,\n    }\n\n\ndef retrieve_text_context(\n    store: FAISS,\n    query: str,\n    company: str,\n    report_year: Optional[int],\n    k: int = 7,\n) -> List[Document]:\n    docs = store.similarity_search(query, k=k)\n    filtered: List[Document] = []\n    for d in docs:\n        meta = d.metadata or {}\n        if company and meta.get(\"company\") != company:\n            continue\n        if report_year is not None and meta.get(\"report_year\") != report_year:\n            continue\n        filtered.append(d)\n    return filtered\n\n\ndef retrieve_tables(\n    store: FAISS,\n    query: str,\n    company: str,\n    report_year: Optional[int],\n    k: int = 12,\n) -> List[Document]:\n    docs = store.similarity_search(query, k=k)\n    filtered: List[Document] = []\n    for d in docs:\n        meta = d.metadata or {}\n        if company and meta.get(\"company\") != company:\n            continue\n        if report_year is not None and meta.get(\"report_year\") != report_year:\n            continue\n        filtered.append(d)\n    return filtered\n\n\ndef score_table_with_text_and_target_year(\n    table_doc: Document,\n    text_docs: List[Document],\n    target_years: List[int],\n    prefer_report_year: Optional[int],\n) -> int:\n    score = 0\n    tmeta = table_doc.metadata or {}\n    ttext = (table_doc.page_content or \"\").lower()\n\n    table_company = tmeta.get(\"company\")\n    table_report_year = tmeta.get(\"report_year\")\n    page_start = tmeta.get(\"page_start\")\n    page_end = tmeta.get(\"page_end\")\n\n    for y in target_years:\n        if str(y) in ttext:\n            score += 4\n\n    if prefer_report_year is not None and table_report_year == prefer_report_year:\n        score += 2\n\n    for td in text_docs:\n        meta = td.metadata or {}\n        text_company = meta.get(\"company\")\n        text_report_year = meta.get(\"report_year\")\n        text_page = meta.get(\"page\")\n\n        if table_company and text_company and table_company == text_company:\n            score += 2\n        if prefer_report_year is not None and text_report_year == prefer_report_year:\n            score += 1\n\n        if isinstance(text_page, int) and isinstance(page_start, int) and isinstance(page_end, int):\n            if page_start <= text_page <= page_end:\n                score += 6\n\n        text_content = (td.page_content or \"\").lower()\n        for kw in NUMERIC_KEYWORDS:\n            if kw in ttext and kw in text_content:\n                score += 1\n\n    return score\n\n\ndef select_best_tables(\n    table_docs: List[Document],\n    text_docs: List[Document],\n    target_years: List[int],\n    prefer_report_year: Optional[int],\n    top_n: int = 2,\n) -> List[Document]:\n    if not table_docs:\n        return []\n    scored: List[Tuple[int, Document]] = []\n    for t in table_docs:\n        scored.append((score_table_with_text_and_target_year(t, text_docs, target_years, prefer_report_year), t))\n    scored.sort(key=lambda x: x[0], reverse=True)\n    return [t for s, t in scored[:top_n] if s > 0]\n\ndef build_csv_snippet(csv_path: Path, user_query: str, target_years: List[int]) -> str:\n    if not csv_path.exists():\n        return f\"[Missing CSV: {csv_path.as_posix()}]\"\n\n    try:\n        df = pd.read_csv(csv_path, dtype=str).fillna(\"\")\n    except Exception as e:\n        return f\"[Failed to read CSV {csv_path.name}: {e}]\"\n\n    df = df.applymap(normalize_text)\n\n    q = user_query.lower()\n    metric_terms = [k for k in NUMERIC_KEYWORDS if k in q]\n    target_year_strs = [str(y) for y in target_years]\n\n    lines: List[str] = []\n    lines.append(f\"CSV_FILE: {csv_path.as_posix()}\")\n    lines.append(\"CSV_TOP_PREVIEW (first 12 rows, first 8 cols):\")\n    lines.append(df.iloc[:12, :8].to_string(index=False))\n\n    if metric_terms:\n        lines.append(\"\\nCSV_METRIC_ROWS (heuristic match, first 8 cols):\")\n        hits = []\n        for i in range(len(df)):\n            row_text = \" | \".join(df.iloc[i, :].tolist()).lower()\n            if any(term in row_text for term in metric_terms):\n                hits.append(i)\n        for i in hits[:8]:\n            lines.append(df.iloc[i:i+1, :8].to_string(index=False))\n\n    if target_year_strs:\n        lines.append(\"\\nCSV_TARGET_YEAR_ROWS (rows containing target year, first 8 cols):\")\n        hits = []\n        for i in range(len(df)):\n            row_text = \" | \".join(df.iloc[i, :].tolist())\n            if any(y in row_text for y in target_year_strs):\n                hits.append(i)\n        for i in hits[:6]:\n            lines.append(df.iloc[i:i+1, :8].to_string(index=False))\n\n    return \"\\n\".join(lines)\n\ndef build_prompt(\n    user_query: str,\n    chat_history: List[Dict[str, str]],\n    text_docs: List[Document],\n    table_docs: List[Document],\n    csv_snippets: List[str],\n    active_company: Optional[str],\n) -> List[Dict[str, str]]:\n    \"\"\"\n    Important: we explicitly tell the model the active company context,\n    so it stops asking \"which company?\" unless truly ambiguous.\n    \"\"\"\n    text_blocks = [\n        f\"[PDF:{d.metadata.get('source_file')} p.{d.metadata.get('page')}] {normalize_text(d.page_content)[:700]}\"\n        for d in text_docs[:10]\n    ]\n\n    table_blocks = [\n        f\"[TABLE:{t.metadata.get('table_id')}] \"\n        f\"company={t.metadata.get('company')} report_year={t.metadata.get('report_year')} \"\n        f\"pages={t.metadata.get('page_start')}-{t.metadata.get('page_end')} \"\n        f\"csv={t.metadata.get('csv_path')}\"\n        for t in table_docs[:6]\n    ]\n\n    system = (\n        \"You are a supportive, professional financial analyst assistant.\\n\"\n        \"Your job is to help the user get to the right answer with a friendly tone.\\n\"\n        \"Rules:\\n\"\n        \"1) Use ONLY the evidence provided.\\n\"\n        \"2) Cite narrative claims using [PDF:FILE p.PAGE].\\n\"\n        \"3) For numeric answers, cite the CSV file path and specify units/currency if visible.\\n\"\n        \"4) If asked to compare multiple years/companies, list the values used before concluding.\\n\"\n        \"5) If evidence is missing, say so and state what is missing instead of guessing. Dont give technical details as which CSV is missing or what is to be done next. \\n\"\n        \"6) If the user does not specify a company, assume the active company context provided.\\n\"\n    )\n\n    messages: List[Dict[str, str]] = [{\"role\": \"system\", \"content\": system}]\n    messages.extend(chat_history[-MAX_CHAT_HISTORY_TURNS:])\n\n    context_line = f\"ACTIVE_COMPANY_CONTEXT: {active_company}\" if active_company else \"ACTIVE_COMPANY_CONTEXT: (none)\"\n\n    user_msg = f\"\"\"\n{context_line}\n\nUser question:\n{user_query}\n\nPDF EVIDENCE:\n{chr(10).join(text_blocks) if text_blocks else \"(none)\"}\n\nTABLE INDEX EVIDENCE:\n{chr(10).join(table_blocks) if table_blocks else \"(none)\"}\n\nCSV EVIDENCE:\n{chr(10).join(csv_snippets) if csv_snippets else \"(none)\"}\n\"\"\"\n    messages.append({\"role\": \"user\", \"content\": user_msg.strip()})\n    return messages\n\n\n\ndef main() -> None:\n    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n\n    text_store = FAISS.load_local(\n        str(FAISS_TEXT_DIR), embeddings, allow_dangerous_deserialization=True\n    )\n    table_store = FAISS.load_local(\n        str(FAISS_TABLES_DIR), embeddings, allow_dangerous_deserialization=True\n    )\n\n    company_report_years = build_company_report_years_map(text_store)\n\n    api_key =  secret_value_0\n    if not api_key:\n        raise EnvironmentError(\"GROQ_API_KEY is not set. Please export GROQ_API_KEY first.\")\n\n    llm = ChatOpenAI(\n        model=GROQ_MODEL_NAME,\n        api_key=api_key,\n        base_url=GROQ_BASE_URL,\n        temperature=0.2,\n    )\n\n    chat_history: List[Dict[str, str]] = []\n    convo_state: Dict[str, Any] = {\n        \"active_company\": None, # current conversation context\n        \"active_target_years\": [], # last referenced years (for follow-ups)\n        \"pending_question\": None,  # last question awaiting company clarification\n    }\n\n    print(\"\\nChatbot ready. Type 'exit' to quit.\\n\")\n\n    while True:\n        user_input = input(\"You----> \").strip()\n        if not user_input:\n            continue\n        if user_input.lower() in {\"exit\", \"quit\"}:\n            break\n\n        context_company = parse_context_only_company(user_input)\n        if context_company:\n            convo_state[\"active_company\"] = context_company\n\n            if convo_state.get(\"pending_question\"):\n                user_query = convo_state[\"pending_question\"]\n                convo_state[\"pending_question\"] = None\n            else:\n                print(f\"\\nAssistant>\\nUnderstood. I’ll assume {context_company} going forward.\\n\")\n                chat_history.append({\"role\": \"user\", \"content\": user_input})\n                chat_history.append({\"role\": \"assistant\", \"content\": f\"Understood. Active company is now {context_company}.\"})\n                continue\n        else:\n            user_query = user_input\n\n\n        rewritten_query = rewrite_query_with_active_context(\n            user_query=user_query,\n            active_company=convo_state.get(\"active_company\"),\n            active_target_years=convo_state.get(\"active_target_years\", []),\n        )\n\n        if not extract_companies(rewritten_query) and not convo_state.get(\"active_company\"):\n            convo_state[\"pending_question\"] = user_query\n            assistant_text = \"Which company is this for (Tesla, BMW, or Ford)?\"\n            print(f\"\\nAssistant>\\n{assistant_text}\\n\")\n            chat_history.append({\"role\": \"user\", \"content\": user_input})\n            chat_history.append({\"role\": \"assistant\", \"content\": assistant_text})\n            continue\n\n        explicit_years = extract_target_years(user_query)\n        if explicit_years:\n            convo_state[\"active_target_years\"] = explicit_years\n\n\n        plan = build_retrieval_plan(\n            user_query=rewritten_query,\n            convo_state=convo_state,\n            company_report_years=company_report_years,\n        )\n\n\n        merged_text_docs: List[Document] = []\n        merged_table_docs: List[Document] = []\n        merged_csv_snippets: List[str] = []\n\n        for company in plan[\"companies\"]:\n            report_years_to_search = plan[\"company_to_report_years\"].get(company, [None])\n\n            for report_year in report_years_to_search:\n                text_docs = retrieve_text_context(text_store, rewritten_query, company, report_year, k=7)\n\n                best_tables: List[Document] = []\n                if plan[\"is_numeric\"]:\n                    table_candidates = retrieve_tables(table_store, rewritten_query, company, report_year, k=12)\n                    best_tables = select_best_tables(\n                        table_docs=table_candidates,\n                        text_docs=text_docs,\n                        target_years=plan[\"target_years\"],\n                        prefer_report_year=report_year,\n                        top_n=2,\n                    )\n\n                    for t in best_tables:\n                        csv_path = Path(t.metadata[\"csv_path\"])\n                        merged_csv_snippets.append(build_csv_snippet(csv_path, rewritten_query, plan[\"target_years\"]))\n\n                merged_text_docs.extend(text_docs)\n                merged_table_docs.extend(best_tables)\n\n        messages = build_prompt(\n            user_query=rewritten_query,\n            chat_history=chat_history,\n            text_docs=merged_text_docs,\n            table_docs=merged_table_docs,\n            csv_snippets=merged_csv_snippets,\n            active_company=convo_state.get(\"active_company\"),\n        )\n\n        try:\n            response_text = llm.invoke(messages).content.strip()\n        except Exception as e:\n            response_text = f\"[ERROR] LLM call failed: {e}\"\n\n        print(f\"\\nAssistant----------->{response_text}\")\n        print(\"--\"*25)\n\n        chat_history.append({\"role\": \"user\", \"content\": user_input})\n        chat_history.append({\"role\": \"assistant\", \"content\": response_text})\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T11:30:40.794306Z","iopub.execute_input":"2025-12-15T11:30:40.794824Z","iopub.status.idle":"2025-12-15T11:42:59.430556Z","shell.execute_reply.started":"2025-12-15T11:30:40.794784Z","shell.execute_reply":"2025-12-15T11:42:59.429343Z"}},"outputs":[{"name":"stdout","text":"\nChatbot ready. Type 'exit' to quit.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  hi\n"},{"name":"stdout","text":"\nAssistant>\nWhich company is this for (Tesla, BMW, or Ford)?\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  BMW\n"},{"name":"stdout","text":"\nAssistant----------->Hello! 👋 It looks like we’re focusing on BMW today. How can I assist you with BMW‑related information or analysis? Feel free to let me know what you’d like to explore!\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  What was its total revenue in 2023?\n"},{"name":"stdout","text":"\nAssistant----------->I’m sorry, but the source that contains BMW’s 2023 total‑revenue figure (the CSV tables BMW_2023_p040_t03 and BMW_2023_p022_t06) is missing, so I can’t provide the exact amount. If you can supply the data file or another source that includes the revenue number, I’ll be happy to give you the answer.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  How much revenue did Tesla generate in 2023?\n"},{"name":"stdout","text":"\nAssistant----------->Tesla generated **$96,773 million** in total revenue for the year ended December 31, 2023【PDF:Tesla_Annual_Report_2023.pdf p.20】.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  What was Ford's revenue for the year 2020?\n"},{"name":"stdout","text":"\nAssistant----------->I’m sorry, but the source that provides Ford’s total revenue for the full year 2020 isn’t included in the evidence you’ve shared. The excerpts we have show segment‑level figures (e.g., Europe $22,644 million and International Markets $7,541 million) but do not give the company‑wide total revenue number. Without that missing data, I can’t provide the exact 2020 revenue figure.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  Can you provide the revenue figures for BMW in 2017?\n"},{"name":"stdout","text":"\nAssistant----------->Sure thing! According to BMW’s 2021 Annual Report, the **group‑wide revenue for 2017 was € 98,282 million**【PDF:BMW_Annual_Report_2021.pdf p.10】.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  What key economic factors influenced Ford's performance in 2021?\n"},{"name":"stdout","text":"\nAssistant----------->I’m sorry, but the evidence you’ve provided (the excerpt from Ford’s 2021 Annual Report about Ford Credit’s public‑term funding) does not contain any discussion of the broader economic factors that affected Ford’s overall performance in 2021. To answer the question about key economic influences—such as supply‑chain constraints, commodity price trends, consumer demand shifts, or macro‑economic conditions—I would need a source that includes Ford’s Management’s Discussion and Analysis or a similar section that addresses those factors. If you can share that portion of the report or another document that covers Ford’s 2021 economic environment, I’ll be happy to summarize the key factors for you.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  Which Tesla product is currently in the development stage?\n"},{"name":"stdout","text":"\nAssistant----------->Tesla currently has two products listed as being **in the development stage**:\n\n| Product | Development Status (as of the 2023 Annual Report) |\n|---------|---------------------------------------------------|\n| **Tesla Roadster** | “In development” |\n| **Next Generation Platform** | “In development” |\n\nThese are the only items shown as “In development” in the production‑status summary of Tesla’s 2023 Form 10‑K【PDF:Tesla_Annual_Report_2023.pdf p.4】.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  What were BMW's profit figures for 2020 and 2023?\n"},{"name":"stdout","text":"\nAssistant----------->I’m sorry, but the sources you’ve provided don’t include BMW’s profit numbers for either 2020 or 2023. The referenced tables that would contain those figures are missing, so I can’t give the profit amounts. If you can share the data from those tables (or another source that lists BMW’s 2020 and 2023 profit), I’ll be happy to report the numbers.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  Between Tesla and Ford, which company achieved higher profits in 2022?\n"},{"name":"stdout","text":"\nAssistant----------->I’m sorry, but the sources you’ve provided do not contain the profit (net‑income) figures for either Tesla or Ford for 2022. The Tesla excerpt lists revenues and costs of revenues, and the Ford excerpt is only a narrative overview; the tables that would show each company’s 2022 profit are missing. Without those profit numbers, I can’t determine which company earned the higher profit in 2022. If you can share the relevant profit data (e.g., the net‑income line from each company’s 2022 consolidated statement of operations), I’ll be happy to compare them for you.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  What were Tesla's profit numbers for 2022 and 2023?\n"},{"name":"stdout","text":"\nAssistant----------->**Tesla – Net Income (Profit)  \n\n| Year | Net income (profit) | Source |\n|------|--------------------|--------|\n| 2022 | **$12,587 million** | 【PDF:Tesla_Annual_Report_2022.pdf p.23】 |\n| 2023 | **$14,974 million** | 【PDF:Tesla_Annual_Report_2023.pdf p.23】 |\n\n*The figures are presented in millions of U.S. dollars, as shown in Tesla’s Consolidated Statements of Cash Flows.*\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  Which company recorded better profitability in 2022 overall?\n"},{"name":"stdout","text":"\nAssistant----------->I’m sorry, but the sources you’ve provided don’t include the profitability figures for BMW in 2022. The tables that would contain the relevant numbers (e.g., net income or operating profit) are missing, so I can’t determine whether BMW recorded better profitability overall in 2022. If you can share the data from those tables or another source that lists BMW’s 2022 profit metrics, I’ll be happy to answer your question.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  im not talking about BMW. I am requesting which company out of Tesla, BMW, Ford  recorded better profitability in 2022 overall?\n"},{"name":"stdout","text":"\nAssistant----------->To compare the overall profitability of the three companies in 2022 we need each firm’s profit (net‑income) figure for that year.\n\n**What we have**\n\n| Company | 2022 profit (net income) | Source |\n|---------|--------------------------|--------|\n| **Tesla** | **$12,587 million** | 【PDF:Tesla_Annual_Report_2022.pdf p.20】 (Consolidated Statements of Operations) |\n| **BMW**   | – (profit figure not provided) | The BMW 2022 annual‑report PDF is listed, but the tables that contain the profit numbers (e.g., BMW_2022_p006_t01 and BMW_2022_p024_t02) are missing from the CSV evidence. |\n| **Ford**  | – (profit figure not provided) | No profit data for Ford 2022 appears in the supplied evidence. |\n\n**Conclusion**\n\n- Tesla’s 2022 profit is known ($12.587 billion).  \n- The profit numbers for BMW and Ford in 2022 are not available in the evidence you’ve shared (the relevant CSV tables are missing, and no narrative profit figures are present).\n\nBecause the necessary profitability data for BMW and Ford are absent, I cannot determine which of the three companies had the highest overall profitability in 2022. If you can provide the missing profit figures (or the CSV files containing them), I’ll be able to complete the comparison for you.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  Provide a summary of revenue figures for Tesla, BMW, and Ford over the past three years.\n"},{"name":"stdout","text":"\nAssistant----------->**Revenue summary for 2022 (as reported in the available documents)**  \n\n| Company | 2022 Revenue | Source |\n|---------|--------------|--------|\n| **Tesla** | **$81,462 million** (total revenues) | 【PDF:Tesla_Annual_Report_2022.pdf p.20】 |\n| **BMW** | – (revenue figure not present in the supplied evidence) | The BMW 2022 annual‑report PDF you provided contains only the cover page; the tables that would list the 2022 revenue (e.g., BMW_2022_p024_t02) are missing. |\n| **Ford** | – (revenue figure not present in the supplied evidence) | No revenue data for Ford 2022 appears in the PDFs or CSV files you shared. |\n\n**What this means**\n\n- For **Tesla**, the 2022 total revenue is clearly stated as $81.462 billion (figures are in millions of U.S. dollars).  \n- For **BMW** and **Ford**, the necessary revenue numbers for 2022 are not available in the current evidence set, so I cannot include them in the summary.\n\nIf you can provide the missing tables or any other source that contains the 2022 revenue numbers for BMW and Ford, I’ll be happy to update the comparison right away.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  What were the growth trends for BMW's financial performance from 2020 to 2023?\n"},{"name":"stdout","text":"\nAssistant----------->I’m sorry, but the evidence you’ve provided does not contain the financial‑performance data (e.g., revenue, operating profit, net income) for BMW for the years 2020, 2021, 2022, or 2023. The tables that would hold those figures— BMW_2023_p004_t03 and BMW_2023_p040_t03—are listed as missing, and no narrative numbers for those years appear in the PDF excerpt.\n\nBecause the required numbers are not available, I can’t describe the growth trends for BMW from 2020 through 2023. If you can supply the missing tables or another source that includes BMW’s 2020‑2023 financial results, I’ll be happy to analyze the trends for you.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You---->  exit\n"}],"execution_count":7}]}